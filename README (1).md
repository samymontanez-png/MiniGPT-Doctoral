# ü§ñ MiniGPT Instructivo en Espa√±ol

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![Transformers](https://img.shields.io/badge/Transformers-4.40+-yellow.svg)](https://huggingface.co/transformers/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

**Proyecto Final de Doctorado en IA/NLP**

Construcci√≥n, Entrenamiento y Evaluaci√≥n de un Modelo GPT Instrucional End-to-End

---

## üìã Descripci√≥n

Este proyecto implementa un modelo de lenguaje tipo GPT (MiniGPT) de **110 millones de par√°metros**, entrenado desde cero en espa√±ol para seguir instrucciones. El modelo fue desarrollado como parte del proyecto final de doctorado en Inteligencia Artificial y Procesamiento del Lenguaje Natural.

### Caracter√≠sticas Principales

- üî§ **Tokenizador BPE personalizado** con 32,000 tokens optimizado para espa√±ol
- üìö **Dataset h√≠brido** de 57,471 instrucciones en espa√±ol
- üß† **Arquitectura GPT-2** (decoder-only) de 110M par√°metros
- üéØ **Fine-tuning con LoRA** para especializaci√≥n en ciencia y programaci√≥n
- üìä **Evaluaci√≥n exhaustiva** con m√©tricas cuantitativas y cualitativas
- üí¨ **Interfaz interactiva** con Gradio

---

## üèóÔ∏è Estructura del Proyecto

```
MiniGPT_Doctoral/
‚îÇ
‚îú‚îÄ‚îÄ üìÅ data/
‚îÇ   ‚îî‚îÄ‚îÄ processed/
‚îÇ       ‚îú‚îÄ‚îÄ train.json              # Dataset de entrenamiento (54,597 ejemplos)
‚îÇ       ‚îî‚îÄ‚îÄ validation.json         # Dataset de validaci√≥n (2,874 ejemplos)
‚îÇ
‚îú‚îÄ‚îÄ üìÅ tokenizer/
‚îÇ   ‚îú‚îÄ‚îÄ bpe_tokenizer.json          # Tokenizador BPE entrenado
‚îÇ   ‚îî‚îÄ‚îÄ hf_tokenizer/               # Formato HuggingFace
‚îÇ       ‚îú‚îÄ‚îÄ tokenizer.json
‚îÇ       ‚îú‚îÄ‚îÄ special_tokens_map.json
‚îÇ       ‚îî‚îÄ‚îÄ tokenizer_config.json
‚îÇ
‚îú‚îÄ‚îÄ üìÅ miniGPT_final/               # Modelo entrenado
‚îÇ   ‚îú‚îÄ‚îÄ config.json
‚îÇ   ‚îú‚îÄ‚îÄ model.safetensors
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer.json
‚îÇ   ‚îî‚îÄ‚îÄ generation_config.json
‚îÇ
‚îú‚îÄ‚îÄ üìÅ miniGPT_lora_ciencia_prog/   # Adaptadores LoRA
‚îÇ   ‚îú‚îÄ‚îÄ adapter_config.json
‚îÇ   ‚îî‚îÄ‚îÄ adapter_model.safetensors
‚îÇ
‚îú‚îÄ‚îÄ üìÅ checkpoints/                 # Checkpoints de entrenamiento
‚îÇ   ‚îú‚îÄ‚îÄ checkpoint-1000/
‚îÇ   ‚îú‚îÄ‚îÄ checkpoint-2000/
‚îÇ   ‚îú‚îÄ‚îÄ checkpoint-3000/
‚îÇ   ‚îú‚îÄ‚îÄ checkpoint-4000/
‚îÇ   ‚îî‚îÄ‚îÄ checkpoint-5000/
‚îÇ
‚îú‚îÄ‚îÄ üìÅ analysis/                    # Resultados y visualizaciones
‚îÇ   ‚îú‚îÄ‚îÄ training_curves.png
‚îÇ   ‚îú‚îÄ‚îÄ benchmark_results.png
‚îÇ   ‚îú‚îÄ‚îÄ coherence_score.png
‚îÇ   ‚îú‚îÄ‚îÄ error_analysis.png
‚îÇ   ‚îú‚îÄ‚îÄ tokenizer_comparison.png
‚îÇ   ‚îî‚îÄ‚îÄ evaluation_report.txt
‚îÇ
‚îú‚îÄ‚îÄ üìÅ notebooks/                   # Notebooks del pipeline
‚îÇ   ‚îú‚îÄ‚îÄ 01_dataset_preparation.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 02_tokenizer_bpe.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 03_encoder_vs_decoder.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 04_training.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 05_evaluation.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ 06_chat_interface.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ 07_lora_finetuning.ipynb
‚îÇ
‚îú‚îÄ‚îÄ chat_interface.py               # Interfaz Gradio standalone
‚îú‚îÄ‚îÄ requirements.txt                # Dependencias
‚îî‚îÄ‚îÄ README.md                       # Este archivo
```

---

## üöÄ Instalaci√≥n

### Requisitos Previos

- Python 3.10 o superior
- CUDA 11.8+ (para entrenamiento con GPU)
- 16GB+ RAM
- GPU con 16GB+ VRAM (recomendado: A100, L4, T4)

### Instalaci√≥n de Dependencias

```bash
# Clonar el repositorio
git clone https://github.com/[tu-usuario]/MiniGPT-Doctoral.git
cd MiniGPT-Doctoral

# Crear entorno virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# Instalar dependencias
pip install -r requirements.txt
```

### requirements.txt

```
torch>=2.0.0
transformers>=4.40.0
datasets>=2.18.0
tokenizers>=0.15.0
accelerate>=0.27.0
peft>=0.10.0
trl>=0.8.0
gradio>=4.0.0
evaluate>=0.4.0
rouge-score>=0.1.2
nltk>=3.8.0
pandas>=2.0.0
numpy>=1.24.0
matplotlib>=3.7.0
seaborn>=0.12.0
scikit-learn>=1.3.0
tqdm>=4.65.0
```

---

## üìä Pipeline del Proyecto

### 1Ô∏è‚É£ Preparaci√≥n del Dataset

```bash
# Ejecutar notebook
jupyter notebook notebooks/01_dataset_preparation.ipynb
```

**Fuentes del dataset:**
- Alpaca Espa√±ol: 51,942 ejemplos
- OpenAssistant ES: 14,038 ejemplos
- Instrucciones originales: 5 ejemplos

**Resultado:** 57,471 instrucciones √∫nicas en formato Alpaca

### 2Ô∏è‚É£ Entrenamiento del Tokenizador BPE

```bash
jupyter notebook notebooks/02_tokenizer_bpe.ipynb
```

**Configuraci√≥n:**
- Vocabulario: 32,000 tokens
- Algoritmo: Byte Pair Encoding
- Eficiencia: 5.10 caracteres/token (75% mejor que GPT-2)

### 3Ô∏è‚É£ Comparaci√≥n Encoder vs Decoder

```bash
jupyter notebook notebooks/03_encoder_vs_decoder.ipynb
```

An√°lisis comparativo entre arquitecturas BERT (encoder-only) y GPT (decoder-only).

### 4Ô∏è‚É£ Entrenamiento del Modelo

```bash
jupyter notebook notebooks/04_training.ipynb
```

**Configuraci√≥n del modelo:**
| Par√°metro | Valor |
|-----------|-------|
| Arquitectura | GPT-2 |
| Par√°metros | 110M |
| Capas | 12 |
| Heads | 12 |
| Embedding | 768 |
| Contexto | 512 tokens |

**Resultados:**
- Steps: 5,121
- Tiempo: 88.6 minutos
- Loss inicial: 9.0654
- Loss final: 3.6201
- Reducci√≥n: 60.1%

### 5Ô∏è‚É£ Evaluaci√≥n

```bash
jupyter notebook notebooks/05_evaluation.ipynb
```

**M√©tricas obtenidas:**
| M√©trica | Valor |
|---------|-------|
| Perplejidad | 80.38 |
| BLEU | 1.44 |
| ROUGE-1 | 0.1710 |
| ROUGE-L | 0.1096 |
| Coherence Score | 0.6931 |

### 6Ô∏è‚É£ Interfaz Interactiva

```bash
# Opci√≥n 1: Notebook
jupyter notebook notebooks/06_chat_interface.ipynb

# Opci√≥n 2: Script standalone
python chat_interface.py
```

### 7Ô∏è‚É£ Fine-tuning con LoRA (Tarea Avanzada)

```bash
jupyter notebook notebooks/07_lora_finetuning.ipynb
```

**Resultados LoRA:**
- Par√°metros entrenables: 2.10%
- Tiempo: 18.6 minutos
- Mejora en perplejidad: 99.1%
- Tama√±o adaptadores: 11.8 MB

---

## üí¨ Uso del Modelo

### Carga B√°sica

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# Cargar modelo y tokenizador
model = AutoModelForCausalLM.from_pretrained("./miniGPT_final")
tokenizer = AutoTokenizer.from_pretrained("./miniGPT_final")

# Generar respuesta
def generate_response(instruction, max_tokens=200):
    prompt = f"### Instrucci√≥n:\n{instruction}\n\n### Respuesta:\n"
    inputs = tokenizer(prompt, return_tensors="pt")
    
    outputs = model.generate(
        **inputs,
        max_new_tokens=max_tokens,
        temperature=0.7,
        top_p=0.9,
        do_sample=True,
        repetition_penalty=1.1
    )
    
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Ejemplo
response = generate_response("¬øQu√© es la inteligencia artificial?")
print(response)
```

### Con Adaptadores LoRA

```python
from peft import PeftModel

# Cargar modelo base
base_model = AutoModelForCausalLM.from_pretrained("./miniGPT_final")

# Cargar adaptadores LoRA
model = PeftModel.from_pretrained(base_model, "./miniGPT_lora_ciencia_prog")

# Usar igual que antes
response = generate_response("Escribe una funci√≥n en Python que calcule el factorial.")
```

### Interfaz Gradio

```bash
python chat_interface.py
# Abre http://localhost:7860 en tu navegador
```

---

## üìà Resultados

### Curvas de Entrenamiento

![Training Curves](analysis/training_curves.png)

### Benchmark por Categor√≠a

![Benchmark Results](analysis/benchmark_results.png)

### Distribuci√≥n de Coherence Score

![Coherence Score](analysis/coherence_score.png)

---

## üìÅ Entregables del Proyecto

| Entregable | Ubicaci√≥n | Estado |
|------------|-----------|--------|
| Memoria t√©cnica (20 p√°gs) | `docs/Informe_Tecnico_MiniGPT.pdf` | ‚úÖ |
| Tokenizador BPE | `tokenizer/` | ‚úÖ |
| Dataset curado | `data/processed/` | ‚úÖ |
| Modelo entrenado | `miniGPT_final/` | ‚úÖ |
| Checkpoints | `checkpoints/` | ‚úÖ |
| Evaluaci√≥n | `analysis/` | ‚úÖ |
| Interfaz | `chat_interface.py` | ‚úÖ |
| LoRA (Tarea Avanzada) | `miniGPT_lora_ciencia_prog/` | ‚úÖ |

---

## üî¨ Tareas Completadas

### Obligatorias

- [x] **Tarea 1:** Construcci√≥n de Tokenizador BPE (32k tokens)
- [x] **Tarea 2:** Comparaci√≥n Encoder-Only vs Decoder-Only
- [x] **Tarea 3:** Entrenamiento de MiniGPT Instructivo
- [x] **Tarea 4:** Evaluaci√≥n Exhaustiva del Modelo
- [x] **Tarea 5:** Interfaz Interactiva (Gradio)

### Avanzadas (S√∫per Distinci√≥n)

- [x] **Tarea B:** Implementaci√≥n de LoRA/QLoRA

---

## üìö Referencias

1. Vaswani, A., et al. (2017). "Attention is All You Need"
2. Radford, A., et al. (2019). "Language Models are Unsupervised Multitask Learners"
3. Brown, T., et al. (2020). "Language Models are Few-Shot Learners"
4. Hu, E., et al. (2021). "LoRA: Low-Rank Adaptation of Large Language Models"
5. Taori, R., et al. (2023). "Stanford Alpaca"

---

## üë§ Autor

**[Tu Nombre]**
- Doctorado en Inteligencia Artificial
- [Tu Universidad]
- Email: [tu@email.com]

---

## üìÑ Licencia

Este proyecto est√° bajo la Licencia MIT. Ver el archivo [LICENSE](LICENSE) para m√°s detalles.

---

## üôè Agradecimientos

- Anthropic por Claude (asistencia en desarrollo)
- HuggingFace por las herramientas de NLP
- Google Colab por recursos de c√≥mputo
- Comunidad de Alpaca Espa√±ol por el dataset base

---

<p align="center">
  <b>Proyecto Final de Doctorado en IA/NLP</b><br>
  Diciembre 2024
</p>
